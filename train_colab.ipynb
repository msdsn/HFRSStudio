{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MOPI-HFRS Training on Google Colab\n",
        "\n",
        "**Multi-Objective Personalized Interpretable Health-aware Food Recommendation System**\n",
        "\n",
        "This notebook trains the MOPI-HFRS model on Google Colab with A100 GPU.\n",
        "\n",
        "## Features:\n",
        "- **CSV or PT file** data loading\n",
        "- **Macro (7 nutrients)** or **All (16 nutrients)** benchmark support\n",
        "- **Pareto multi-objective optimization** (BPR + Health + Diversity)\n",
        "- **Health-aware graph structure learning**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup Environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU\n",
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "# PyTorch 2.8.0 is needed for torch-sparse/scatter compatibility\n",
        "%pip install torch==2.8.0 torchvision torchaudio -q\n",
        "%pip install torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-2.8.0+cu121.html -q\n",
        "%pip install torch-geometric -q\n",
        "%pip install tqdm pandas scikit-learn -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive (if data is stored there)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Copy project files from Drive to Colab\n",
        "# Adjust the path according to your Drive structure\n",
        "!cp -r /content/drive/MyDrive/HFRSStudio /content/\n",
        "\n",
        "# Set working directory\n",
        "import os\n",
        "os.chdir('/content/HFRSStudio')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Import Libraries and Check Environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, '/content/HFRSStudio')\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# Check PyTorch and CUDA\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from config import get_colab_config\n",
        "from data.data_loader import HFRSDataset, HFRSDatasetFromPT\n",
        "from models.mopi_hfrs import create_model\n",
        "from train import train, set_seed\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Configure Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# DATA CONFIGURATION\n",
        "# ============================================================\n",
        "\n",
        "# Data source: CSV files or pre-processed PT file\n",
        "USE_CSV = True  # Set to True to load from CSV files, False for PT file\n",
        "\n",
        "# Benchmark type: 'macro' (7 nutrients) or 'all' (16 nutrients)\n",
        "# - macro: calories, carbohydrates, protein, saturated fat, cholesterol, sugar, dietary fiber\n",
        "# - all: macro + sodium, potassium, phosphorus, iron, calcium, folic acid, vitamin C, D, B12\n",
        "BENCHMARK_TYPE = 'macro'\n",
        "\n",
        "# Data paths\n",
        "DATA_DIR = '/content/drive/MyDrive/MOPI-HFRS_gdrive/processed_data'\n",
        "PT_FILE = f'{DATA_DIR}/benchmark_{BENCHMARK_TYPE}.pt'  # Only used if USE_CSV=False\n",
        "\n",
        "# ============================================================\n",
        "# TRAINING CONFIGURATION  \n",
        "# ============================================================\n",
        "\n",
        "config = get_colab_config()\n",
        "\n",
        "# Data settings\n",
        "config.data.data_dir = DATA_DIR\n",
        "config.data.benchmark_type = BENCHMARK_TYPE\n",
        "config.data.use_pt_file = not USE_CSV\n",
        "config.data.pt_file = PT_FILE\n",
        "config.data.train_ratio = 0.6\n",
        "config.data.val_ratio = 0.2\n",
        "\n",
        "# Training settings\n",
        "config.training.epochs = 500\n",
        "config.training.batch_size = 4096  # Large batch for A100\n",
        "config.training.learning_rate = 1e-3\n",
        "config.training.eval_every = 25\n",
        "config.training.save_dir = '/content/drive/MyDrive/checkpoints/mopi_hfrs'\n",
        "config.training.seed = 42\n",
        "\n",
        "# Model settings\n",
        "config.model.embedding_dim = 128\n",
        "config.model.num_layers = 3\n",
        "config.model.num_heads = 4\n",
        "config.model.feature_threshold = 0.3\n",
        "\n",
        "# Print configuration\n",
        "print(\"=\"*60)\n",
        "print(\"CONFIGURATION\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Data source: {'CSV files' if USE_CSV else 'PT file'}\")\n",
        "print(f\"Benchmark type: {BENCHMARK_TYPE}\")\n",
        "print(f\"Data dir: {DATA_DIR}\")\n",
        "print(f\"Train/Val/Test: {config.data.train_ratio}/{config.data.val_ratio}/{1-config.data.train_ratio-config.data.val_ratio}\")\n",
        "print()\n",
        "print(f\"Epochs: {config.training.epochs}\")\n",
        "print(f\"Batch size: {config.training.batch_size}\")\n",
        "print(f\"Learning rate: {config.training.learning_rate}\")\n",
        "print()\n",
        "print(f\"Embedding dim: {config.model.embedding_dim}\")\n",
        "print(f\"Num layers: {config.model.num_layers}\")\n",
        "print(f\"Num heads: {config.model.num_heads}\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "print(\"Loading dataset...\")\n",
        "start_time = time.time()\n",
        "\n",
        "if USE_CSV:\n",
        "    # Load from CSV files\n",
        "    dataset = HFRSDataset(\n",
        "        data_dir=DATA_DIR,\n",
        "        train_ratio=config.data.train_ratio,\n",
        "        val_ratio=config.data.val_ratio,\n",
        "        seed=config.training.seed,\n",
        "        normalize=config.data.normalize_features,\n",
        "        benchmark_type=BENCHMARK_TYPE\n",
        "    )\n",
        "else:\n",
        "    # Load from PT file\n",
        "    dataset = HFRSDatasetFromPT(\n",
        "        pt_file=PT_FILE,\n",
        "        train_ratio=config.data.train_ratio,\n",
        "        val_ratio=config.data.val_ratio,\n",
        "        seed=config.training.seed\n",
        "    )\n",
        "\n",
        "load_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\nDataset loaded in {load_time:.1f} seconds\")\n",
        "print(f\"  Users: {dataset.num_users:,}\")\n",
        "print(f\"  Foods: {dataset.num_foods:,}\")\n",
        "print(f\"  User features: {dataset.user_features.shape}\")\n",
        "print(f\"  Food features: {dataset.food_features.shape}\")\n",
        "print(f\"  Train edges: {dataset.splits['train_edge_index'].shape[1]:,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Create Model and Train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from models.mopi_hfrs import MOPI_HFRS\n",
        "from utils.losses import bpr_loss, health_loss, diversity_loss\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import os\n",
        "\n",
        "# Create model\n",
        "model = MOPI_HFRS(\n",
        "    num_users=dataset.num_users,\n",
        "    num_foods=dataset.num_foods,\n",
        "    user_feature_dim=dataset.user_features.shape[1],\n",
        "    food_feature_dim=dataset.food_features.shape[1],\n",
        "    embedding_dim=config.model.embedding_dim,\n",
        "    num_layers=config.model.num_layers,\n",
        "    num_heads=config.model.num_heads,\n",
        "    feature_threshold=config.model.feature_threshold\n",
        ")\n",
        "\n",
        "# Move to GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "dataset = dataset.to(device)\n",
        "\n",
        "# Count parameters\n",
        "num_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Model created on {device}\")\n",
        "print(f\"  Total parameters: {num_params:,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set seed for reproducibility\n",
        "torch.manual_seed(config.training.seed)\n",
        "np.random.seed(config.training.seed)\n",
        "random.seed(config.training.seed)\n",
        "\n",
        "# Create save directory\n",
        "os.makedirs(config.training.save_dir, exist_ok=True)\n",
        "\n",
        "# Optimizer and scheduler\n",
        "optimizer = optim.Adam(model.parameters(), lr=config.training.learning_rate, weight_decay=config.training.weight_decay)\n",
        "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=config.training.lr_decay_gamma)\n",
        "\n",
        "# Get data tensors\n",
        "feature_dict = dataset.get_feature_dict()\n",
        "train_edge_index = dataset.splits['train_edge_index']\n",
        "val_edge_index = dataset.splits['val_edge_index']\n",
        "test_edge_index = dataset.splits['test_edge_index']\n",
        "\n",
        "# Get positive/negative edges for signed graph learning\n",
        "pos_edge_index = dataset.graph['user', 'eats', 'food'].pos_edge_index.to(device)\n",
        "neg_edge_index = dataset.graph['user', 'eats', 'food'].neg_edge_index.to(device)\n",
        "\n",
        "# Get tags for health loss\n",
        "user_tags = dataset.user_tags\n",
        "food_tags = dataset.food_tags\n",
        "food_features = dataset.food_features\n",
        "\n",
        "print(f\"Train edges: {train_edge_index.shape[1]:,}\")\n",
        "print(f\"Pos edges: {pos_edge_index.shape[1]:,}\")\n",
        "print(f\"Neg edges: {neg_edge_index.shape[1]:,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sample_mini_batch(batch_size, edge_index, num_items):\n",
        "    \"\"\"Sample a mini-batch for training.\"\"\"\n",
        "    num_edges = edge_index.shape[1]\n",
        "    indices = torch.randperm(num_edges, device=edge_index.device)[:batch_size]\n",
        "    \n",
        "    user_indices = edge_index[0, indices]\n",
        "    pos_item_indices = edge_index[1, indices]\n",
        "    neg_item_indices = torch.randint(0, num_items, (batch_size,), device=edge_index.device)\n",
        "    \n",
        "    return user_indices, pos_item_indices, neg_item_indices\n",
        "\n",
        "\n",
        "# Training loop\n",
        "best_val_loss = float('inf')\n",
        "train_losses = []\n",
        "\n",
        "print(\"\\nStarting training...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for epoch in tqdm(range(1, config.training.epochs + 1), desc=\"Training\"):\n",
        "    model.train()\n",
        "    \n",
        "    # Forward pass\n",
        "    users_emb_final, users_emb_0, items_emb_final, items_emb_0 = model(\n",
        "        feature_dict, train_edge_index, pos_edge_index, neg_edge_index\n",
        "    )\n",
        "    \n",
        "    # Sample mini-batch\n",
        "    user_indices, pos_item_indices, neg_item_indices = sample_mini_batch(\n",
        "        config.training.batch_size, train_edge_index, dataset.num_foods\n",
        "    )\n",
        "    \n",
        "    # Get batch embeddings\n",
        "    users_batch = users_emb_final[user_indices]\n",
        "    users_0_batch = users_emb_0[user_indices]\n",
        "    pos_items_batch = items_emb_final[pos_item_indices]\n",
        "    pos_items_0_batch = items_emb_0[pos_item_indices]\n",
        "    neg_items_batch = items_emb_final[neg_item_indices]\n",
        "    neg_items_0_batch = items_emb_0[neg_item_indices]\n",
        "    \n",
        "    # Get batch tags\n",
        "    user_tags_batch = user_tags[user_indices]\n",
        "    pos_item_tags_batch = food_tags[pos_item_indices]\n",
        "    neg_item_tags_batch = food_tags[neg_item_indices]\n",
        "    \n",
        "    # Get batch features for diversity\n",
        "    pos_item_features_batch = food_features[pos_item_indices]\n",
        "    \n",
        "    # Compute losses\n",
        "    loss_bpr = bpr_loss(\n",
        "        users_batch, users_0_batch,\n",
        "        pos_items_batch, pos_items_0_batch,\n",
        "        neg_items_batch, neg_items_0_batch,\n",
        "        lambda_val=config.training.lambda_val\n",
        "    )\n",
        "    \n",
        "    loss_health = health_loss(\n",
        "        user_tags_batch, pos_item_tags_batch, neg_item_tags_batch,\n",
        "        users_batch, pos_items_batch, neg_items_batch\n",
        "    )\n",
        "    \n",
        "    loss_div = diversity_loss(\n",
        "        users_batch, pos_items_batch, pos_item_features_batch\n",
        "    )\n",
        "    \n",
        "    # Combined loss (Pareto optimization can be added later)\n",
        "    total_loss = loss_bpr + 0.1 * loss_health + 0.1 * loss_div\n",
        "    \n",
        "    # Backward\n",
        "    optimizer.zero_grad()\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    train_losses.append(total_loss.item())\n",
        "    \n",
        "    # Evaluation\n",
        "    if epoch % config.training.eval_every == 0:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_users_emb, _, val_items_emb, _ = model(\n",
        "                feature_dict, val_edge_index, pos_edge_index, neg_edge_index\n",
        "            )\n",
        "            val_user_idx = val_edge_index[0]\n",
        "            val_item_idx = val_edge_index[1]\n",
        "            val_scores = (val_users_emb[val_user_idx] * val_items_emb[val_item_idx]).sum(dim=1)\n",
        "            val_loss = -torch.mean(torch.log(torch.sigmoid(val_scores) + 1e-8)).item()\n",
        "        \n",
        "        print(f\"\\nEpoch {epoch}: Train={total_loss.item():.4f}, \"\n",
        "              f\"BPR={loss_bpr.item():.4f}, Health={loss_health.item():.4f}, \"\n",
        "              f\"Div={loss_div.item():.4f}, Val={val_loss:.4f}\")\n",
        "        \n",
        "        # Save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_loss': val_loss,\n",
        "            }, f\"{config.training.save_dir}/best_model.pt\")\n",
        "            print(f\"  -> Saved best model (val_loss={best_val_loss:.4f})\")\n",
        "    \n",
        "    # Learning rate decay\n",
        "    if epoch % config.training.lr_decay_step == 0:\n",
        "        scheduler.step()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Training completed!\")\n",
        "print(f\"Best validation loss: {best_val_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Generate Recommendations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load best model\n",
        "checkpoint = torch.load(f\"{config.training.save_dir}/best_model.pt\")\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n",
        "\n",
        "print(f\"Loaded best model from epoch {checkpoint['epoch']}\")\n",
        "\n",
        "# Generate embeddings\n",
        "with torch.no_grad():\n",
        "    users_emb, items_emb = model.get_embeddings(\n",
        "        feature_dict, train_edge_index, pos_edge_index, neg_edge_index\n",
        "    )\n",
        "\n",
        "print(f\"User embeddings: {users_emb.shape}\")\n",
        "print(f\"Item embeddings: {items_emb.shape}\")\n",
        "\n",
        "# Generate top-10 recommendations for sample users\n",
        "print(\"\\nSample Recommendations:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for user_idx in [0, 100, 1000, 5000, 10000]:\n",
        "    if user_idx >= dataset.num_users:\n",
        "        continue\n",
        "        \n",
        "    top_k_items, top_k_scores = model.recommend(\n",
        "        user_idx, users_emb, items_emb, k=10\n",
        "    )\n",
        "    \n",
        "    print(f\"\\nUser {user_idx}:\")\n",
        "    for i, (item_idx, score) in enumerate(zip(top_k_items, top_k_scores)):\n",
        "        print(f\"  {i+1}. Food {item_idx.item()}: score={score.item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
